{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1qpQ04hvt5QJEpT4A4A2NqrtRyGh-d9T1",
      "authorship_tag": "ABX9TyNFux9iGY9R7zkQhDOwjtVn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "Let's get this out of the way up front!\n",
        "\n",
        "**Note: Run with GPU environment!**\n",
        "\n",
        "> Click `Runtime >> Change runtime type >> GPU`\n",
        "\n",
        "I think GPU is way faster than TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY",
        "outputId": "9005487e-9e91-4274-c611-61ec94157bd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.6MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if57ccVO5xrc"
      },
      "source": [
        "# Download Wikipedia Articles\n",
        "First, we need a corpus of relatively clean data. Wikipedia is crowd-sourced and written in modern English. Therefore we can trust that it is a good source of semantically, syntactically, and rhetorically sound text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqvg2ce5tAE",
        "outputId": "40eeaafe-50ce-42f1-a4d6-8034d8957148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "# todo: come up with a cool way to automatically create topic search terms\n",
        "keywords = ['india', 'ocean', 'astronomy', 'economics', 'economy', 'earth', \n",
        "            'english', 'bacon', 'egg', 'dinosaur', 'rabbit', 'america', 'usa',\n",
        "            'congress', 'virus', 'George Clooney', 'knowledge', 'Buddha']\n",
        "\n",
        "def save_article(title, article):\n",
        "  with open('wiki_' + title + '.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(article)\n",
        "\n",
        "for keyword in keywords:\n",
        "  print('Searching Wikipedia for keyword:', keyword)\n",
        "  try:\n",
        "    search = wikipedia.search(keyword)\n",
        "    for result in search:\n",
        "      article = wikipedia.page(result)\n",
        "      save_article(result, article.content)\n",
        "  except Exception as oops:\n",
        "    continue\n",
        "print('Done saving articles!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching Wikipedia for keyword: india\n",
            "Searching Wikipedia for keyword: ocean\n",
            "Searching Wikipedia for keyword: astronomy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Searching Wikipedia for keyword: economics\n",
            "Searching Wikipedia for keyword: economy\n",
            "Searching Wikipedia for keyword: earth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9KOoQu-_4eu"
      },
      "source": [
        "# Parse Articles\n",
        "The articles need to be split up into usable chunks. This uses regex to identify the section headers and split each article into single lines of text for each section. Furthermore, it looks at the number of word characters vs other characters to identify those sections that likely contain text instead of tables or other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPyoXJkhABpH"
      },
      "source": [
        "import os \n",
        "import re\n",
        "\n",
        "result = list()\n",
        "\n",
        "for file in os.listdir('.'):\n",
        "  if not 'wiki_' in file:\n",
        "    continue\n",
        "  with open(file, 'r', encoding='utf-8') as infile:\n",
        "    text = infile.read()\n",
        "  sections = re.split(r'={2,}.{0,80}={2,}', text)\n",
        "  for section in sections:\n",
        "    try:\n",
        "      trimmed = section.strip()\n",
        "      wordchars = re.findall(r'\\w', trimmed)\n",
        "      ratio = len(wordchars) / len(trimmed)\n",
        "      # it seems like a ratio of greater than 80% word chars is ideal\n",
        "      if ratio > 0.80:\n",
        "        final = re.sub(r'\\s+', ' ', trimmed)\n",
        "        result.append(final)\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "print('Wikipedia sections parsed:', len(result))\n",
        "with open('wikiparsed.txt', 'w', encoding='utf-8') as outfile:\n",
        "  for line in result:\n",
        "    outfile.write(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R27VsKBGKF"
      },
      "source": [
        "# Split Sentences\n",
        "For the sake of simplicity, we don't want to go overboard and evaluate entire paragraphs. We want to only train on individual sentences. So let's use SpaCy and PYSBD (Python Sentence Boundary Detector) to split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlEJaDTQBkw9"
      },
      "source": [
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "infile = 'wikiparsed.txt'\n",
        "outfile = 'wikisentences.txt'\n",
        "result = list()\n",
        "\n",
        "with open('wikiparsed.txt', 'r', encoding='utf-8') as infile:\n",
        "  lines = infile.readlines()\n",
        "\n",
        "print('Lines of text:', len(lines))\n",
        "for line in lines:\n",
        "  doc = nlp(line)\n",
        "  for sent in list(doc.sents):\n",
        "    result.append(sent)\n",
        "\n",
        "print('Sentences found:', len(result))\n",
        "with open('wikisentences.txt', 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    if str(line) == '':\n",
        "      continue\n",
        "    file.write(str(line)+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4yrVXwCdLs"
      },
      "source": [
        "# Generate Gibberish v1\n",
        "### Scrambled Words\n",
        "We have a great source of sentences that are semantically, syntactically, and rhetorically sound. The simplest way to generate gibberish, then, would be to scramble these sentences! For this first version, we want words, just all mixed up. This will create good training data because the samples will contain the same exact words as the sound sentences but out of order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8BeEYOC14m"
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  split = sentence.split()\n",
        "  shuffle(split)\n",
        "  return ' '.join(split)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29uugV1DPN_"
      },
      "source": [
        "# Generate Gibberish v2\n",
        "### Completely Random Characters\n",
        "This step may not be necessary but I'd like to be able to detect utter nonsense as well. So let's scramble all the characters in each sentence completely. I figure it's better to show the model random noise as well as random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJxLvqqDgIV"
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled2.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  sentence = list(sentence)\n",
        "  shuffle(sentence)\n",
        "  return ''.join(sentence)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "Let's build a training corpus that we can feed to GPT2! We need to bake the label directly into each line. Change `max_samples` to adjust corpus size. Multiple trainings may be necessary. Limits to finetuning memory requirements. I will add updates about limits and constraints as I figure them out. \n",
        "\n",
        "I'm afraid that this will just learn to pay attention to caps and periods so I might change the way the final corpus looks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk"
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "files = [\n",
        "('wikisentences.txt', 'clean'), \n",
        "#('wikiscrambled2.txt', 'gibberish'),  # excluding complete noise for now\n",
        "('wikiscrambled.txt', 'gibberish')\n",
        "]\n",
        "\n",
        "result = list()\n",
        "max_samples = 5000  # the max here is the number of sentences from above\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = line.lower().replace('.', '')  # this will make it harder to cheat\n",
        "    line = '// %s || %s ' % (line, file[1])\n",
        "    result.append(line)\n",
        "\n",
        "#seed()\n",
        "#subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in result:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbtvF1mgwJAx"
      },
      "source": [
        "# Load Model\n",
        "Let's use Google Drive to store the model for persistence. We will want to fine tune the model iteratively to get better and better performance. We will also want to use the model again later after pouring so much work into it!\n",
        "\n",
        "Information about [download_gpt2 function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHmW7pU-wey0"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "gpt2.download_gpt2(model_name='355M', model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Finetune GPT2!\n",
        "This is where the rubber meets the road! Let's see if we can finetune a GPT-2 model! Obviously, the bigger the model, the better the results. But bigger models require more memory. There's a tradeoff between model size and corpus size. It looks like 355M is the largest model we can do for now. \n",
        "\n",
        "[Finetune function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L127)\n",
        "\n",
        "Run this repeatedly with more/different training data to get better results.\n",
        "\n",
        "Simplest way to continue training is to click `Runtime >> Restart and run all...`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P5VZm4-eec"
      },
      "source": [
        "file_name = 'corpus.txt'\n",
        "sess = gpt2.start_tf_sess()\n",
        "run_name = 'GibberishDetector'\n",
        "model_name = '355M'\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=1000,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=100\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8q_9FuxDg2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0DKLLTp0mp"
      },
      "source": [
        "# Generate with GPT-2\n",
        "Let's make sure we can save and load the model that we worked so hard on!\n",
        "\n",
        "[Generation information here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L407)\n",
        "\n",
        "Run this after training a model and restarting the instance. This will demonstrate that the model is saved and working."
      ]
    }
  ]
}