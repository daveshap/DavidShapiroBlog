{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/daveshap/GibberishDetector/blob/main/GibberishDetector.ipynb",
      "authorship_tag": "ABX9TyMRco1TRNoeAp8/olaCv6Hg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/DavidShapiroBlog/blob/master/GibberishDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "Let's get this out of the way up front!\n",
        "\n",
        "**Note: Run with GPU environment!**\n",
        "\n",
        "> Click `Runtime >> Change runtime type >> GPU`\n",
        "\n",
        "I think GPU is way faster than TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY",
        "outputId": "ecb74b18-7511-4ad7-a9fa-6454c17e4161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 411.5MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 36.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if57ccVO5xrc"
      },
      "source": [
        "# Download Wikipedia Articles\n",
        "First, we need a corpus of relatively clean data. Wikipedia is crowd-sourced and written in modern English. Therefore we can trust that it is a good source of semantically, syntactically, and rhetorically sound text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqvg2ce5tAE",
        "outputId": "61699694-39f0-4649-b9b3-069e0c090363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "# todo: come up with a cool way to automatically create topic search terms\n",
        "keywords = ['india', 'ocean', 'astronomy', 'economics', 'economy', 'earth', \n",
        "            'english', 'bacon', 'egg', 'dinosaur', 'rabbit', 'america', 'usa',\n",
        "            'congress', 'virus', 'George Clooney', 'knowledge', 'Buddha']\n",
        "\n",
        "def save_article(title, article):\n",
        "  with open('wiki_' + title + '.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(article)\n",
        "\n",
        "for keyword in keywords:\n",
        "  print('Searching Wikipedia for keyword:', keyword)\n",
        "  try:\n",
        "    search = wikipedia.search(keyword)\n",
        "    for result in search:\n",
        "      article = wikipedia.page(result)\n",
        "      save_article(result, article.content)\n",
        "  except Exception as oops:\n",
        "    continue\n",
        "print('Done saving articles!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching Wikipedia for keyword: india\n",
            "Searching Wikipedia for keyword: ocean\n",
            "Searching Wikipedia for keyword: astronomy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Searching Wikipedia for keyword: economics\n",
            "Searching Wikipedia for keyword: economy\n",
            "Searching Wikipedia for keyword: earth\n",
            "Searching Wikipedia for keyword: english\n",
            "Searching Wikipedia for keyword: bacon\n",
            "Searching Wikipedia for keyword: egg\n",
            "Searching Wikipedia for keyword: dinosaur\n",
            "Searching Wikipedia for keyword: rabbit\n",
            "Searching Wikipedia for keyword: america\n",
            "Searching Wikipedia for keyword: usa\n",
            "Searching Wikipedia for keyword: congress\n",
            "Searching Wikipedia for keyword: virus\n",
            "Searching Wikipedia for keyword: George Clooney\n",
            "Searching Wikipedia for keyword: knowledge\n",
            "Searching Wikipedia for keyword: Buddha\n",
            "Done saving articles!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9KOoQu-_4eu"
      },
      "source": [
        "# Parse Articles\n",
        "The articles need to be split up into usable chunks. This uses regex to identify the section headers and split each article into single lines of text for each section. Furthermore, it looks at the number of word characters vs other characters to identify those sections that likely contain text instead of tables or other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPyoXJkhABpH",
        "outputId": "c143a48b-b3bd-45aa-ccf4-be348e865dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os \n",
        "import re\n",
        "\n",
        "result = list()\n",
        "\n",
        "for file in os.listdir('.'):\n",
        "  if not 'wiki_' in file:\n",
        "    continue\n",
        "  with open(file, 'r', encoding='utf-8') as infile:\n",
        "    text = infile.read()\n",
        "  sections = re.split(r'={2,}.{0,80}={2,}', text)\n",
        "  for section in sections:\n",
        "    try:\n",
        "      trimmed = section.strip()\n",
        "      wordchars = re.findall(r'\\w', trimmed)\n",
        "      ratio = len(wordchars) / len(trimmed)\n",
        "      # it seems like a ratio of greater than 80% word chars is ideal\n",
        "      if ratio > 0.80:\n",
        "        final = re.sub(r'\\s+', ' ', trimmed)\n",
        "        result.append(final)\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "print('Wikipedia sections parsed:', len(result))\n",
        "with open('wikiparsed.txt', 'w', encoding='utf-8') as outfile:\n",
        "  for line in result:\n",
        "    outfile.write(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wikipedia sections parsed: 1651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R27VsKBGKF"
      },
      "source": [
        "# Split Sentences\n",
        "For the sake of simplicity, we don't want to go overboard and evaluate entire paragraphs. We want to only train on individual sentences. So let's use SpaCy and PYSBD (Python Sentence Boundary Detector) to split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlEJaDTQBkw9",
        "outputId": "690e3f5b-3631-42d3-931c-81ebd4908514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "infile = 'wikiparsed.txt'\n",
        "outfile = 'wikisentences.txt'\n",
        "result = list()\n",
        "\n",
        "with open('wikiparsed.txt', 'r', encoding='utf-8') as infile:\n",
        "  lines = infile.readlines()\n",
        "\n",
        "print('Lines of text:', len(lines))\n",
        "for line in lines:\n",
        "  doc = nlp(line)\n",
        "  for sent in list(doc.sents):\n",
        "    result.append(sent)\n",
        "\n",
        "print('Sentences found:', len(result))\n",
        "with open('wikisentences.txt', 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    if str(line) == '':\n",
        "      continue\n",
        "    file.write(str(line)+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lines of text: 1651\n",
            "Sentences found: 14955\n",
            "wikisentences.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4yrVXwCdLs"
      },
      "source": [
        "# Generate Gibberish v1\n",
        "### Scrambled Words\n",
        "We have a great source of sentences that are semantically, syntactically, and rhetorically sound. The simplest way to generate gibberish, then, would be to scramble these sentences! For this first version, we want words, just all mixed up. This will create good training data because the samples will contain the same exact words as the sound sentences but out of order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8BeEYOC14m",
        "outputId": "dd489ad7-d2d1-4a0c-e132-fea670c3c04f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  split = sentence.split()\n",
        "  shuffle(split)\n",
        "  return ' '.join(split)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikiscrambled.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29uugV1DPN_"
      },
      "source": [
        "# Generate Gibberish v2\n",
        "### Completely Random Characters\n",
        "This step may not be necessary but I'd like to be able to detect utter nonsense as well. So let's scramble all the characters in each sentence completely. I figure it's better to show the model random noise as well as random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJxLvqqDgIV",
        "outputId": "476f3b12-5d2f-4c24-bc93-68da751912a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled2.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  sentence = list(sentence)\n",
        "  shuffle(sentence)\n",
        "  return ''.join(sentence)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikiscrambled2.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "Let's build a training corpus that we can feed to GPT2! We need to bake the label directly into each line. Change `max_samples` to adjust corpus size. Multiple trainings may be necessary. Limits to finetuning memory requirements. I will add updates about limits and constraints as I figure them out. \n",
        "\n",
        "I'm afraid that this will just learn to pay attention to caps and periods so I might change the way the final corpus looks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk",
        "outputId": "100ff648-3ee2-49a7-cd42-eae2ee78b9de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "files = [\n",
        "('wikisentences.txt', 'clean'), \n",
        "#('wikiscrambled2.txt', 'gibberish'),  # excluding complete noise for now\n",
        "('wikiscrambled.txt', 'gibberish')\n",
        "]\n",
        "\n",
        "result = list()\n",
        "max_samples = 5000  # the max here is the number of sentences from above\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = line.lower().replace('.', '')  # this will make it harder to cheat\n",
        "    line = '// %s || %s ' % (line, file[1])\n",
        "    result.append(line)\n",
        "\n",
        "#seed()\n",
        "#subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in result:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbtvF1mgwJAx"
      },
      "source": [
        "# Load Model\n",
        "Let's use Google Drive to store the model for persistence. We will want to fine tune the model iteratively to get better and better performance. We will also want to use the model again later after pouring so much work into it!\n",
        "\n",
        "Information about [download_gpt2 function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHmW7pU-wey0",
        "outputId": "bc24d2fa-53e4-4b3e-f24c-d1b7a72f880b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "gpt2.download_gpt2(model_name='355M', model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 222Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 103Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 284Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:10, 129Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 240Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 94.2Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 141Mit/s]                                                       "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model is ready!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Finetune GPT2!\n",
        "This is where the rubber meets the road! Let's see if we can finetune a GPT-2 model! Obviously, the bigger the model, the better the results. But bigger models require more memory. There's a tradeoff between model size and corpus size. It looks like 355M is the largest model we can do for now. \n",
        "\n",
        "[Finetune function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L127)\n",
        "\n",
        "Run this repeatedly with more/different training data to get better results.\n",
        "\n",
        "Simplest way to continue training is to click `Runtime >> Restart and run all...`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P5VZm4-eec",
        "outputId": "bd280721-c3c3-421a-e3b5-d799233b3822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_name = 'corpus.txt'\n",
        "sess = gpt2.start_tf_sess()\n",
        "run_name = 'GibberishDetector'\n",
        "model_name = '355M'\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=1000,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=100\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint /content/drive/My Drive/GPT2/models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/GPT2/models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.44s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1480945 tokens\n",
            "Training...\n",
            "[5 | 12.44] loss=2.45 avg=2.45\n",
            "[10 | 15.03] loss=2.66 avg=2.55\n",
            "[15 | 17.64] loss=2.14 avg=2.41\n",
            "[20 | 20.24] loss=3.95 avg=2.80\n",
            "[25 | 22.85] loss=4.42 avg=3.13\n",
            "[30 | 25.48] loss=1.87 avg=2.92\n",
            "[35 | 28.07] loss=2.63 avg=2.87\n",
            "[40 | 30.67] loss=4.37 avg=3.07\n",
            "[45 | 33.26] loss=3.87 avg=3.16\n",
            "[50 | 35.86] loss=2.23 avg=3.06\n",
            "[55 | 38.47] loss=2.79 avg=3.04\n",
            "[60 | 41.06] loss=3.91 avg=3.12\n",
            "[65 | 43.68] loss=4.19 avg=3.20\n",
            "[70 | 46.29] loss=2.52 avg=3.15\n",
            "[75 | 48.90] loss=4.44 avg=3.24\n",
            "[80 | 51.51] loss=4.71 avg=3.34\n",
            "[85 | 54.11] loss=2.39 avg=3.28\n",
            "[90 | 56.71] loss=2.15 avg=3.21\n",
            "[95 | 59.31] loss=2.19 avg=3.15\n",
            "[100 | 61.92] loss=4.20 avg=3.21\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-100\n",
            "[105 | 72.27] loss=3.96 avg=3.25\n",
            "[110 | 74.89] loss=4.04 avg=3.29\n",
            "[115 | 77.50] loss=1.94 avg=3.23\n",
            "[120 | 80.10] loss=4.06 avg=3.26\n",
            "[125 | 82.72] loss=3.84 avg=3.29\n",
            "[130 | 85.35] loss=4.05 avg=3.32\n",
            "[135 | 87.96] loss=3.77 avg=3.34\n",
            "[140 | 90.56] loss=2.55 avg=3.31\n",
            "[145 | 93.15] loss=4.10 avg=3.34\n",
            "[150 | 95.76] loss=2.23 avg=3.30\n",
            "[155 | 98.37] loss=2.04 avg=3.25\n",
            "[160 | 100.99] loss=3.74 avg=3.27\n",
            "[165 | 103.61] loss=1.82 avg=3.22\n",
            "[170 | 106.23] loss=2.16 avg=3.18\n",
            "[175 | 108.84] loss=2.23 avg=3.15\n",
            "[180 | 111.45] loss=1.62 avg=3.10\n",
            "[185 | 114.06] loss=4.28 avg=3.14\n",
            "[190 | 116.67] loss=2.39 avg=3.11\n",
            "[195 | 119.28] loss=1.81 avg=3.07\n",
            "[200 | 121.88] loss=3.56 avg=3.09\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            " of the second century and the last four centuries of chinese or chinese nationalism, chinese intellectuals were the first to develop a systematic understanding of the chinese identity (eccleston 1998:12)\n",
            "\n",
            "chinese nationalism was primarily a political identity movement that appealed to the broadest of \"chinese\" cultural and racial groups (fong 1988:21)\n",
            "\n",
            "chinese nationalist movements also included the creation and spread of a multitude of political movements, and many leaders also promoted a variety of political positions (eccleston 1998:5)\n",
            "\n",
            "the creation of the modern movement itself has been labeled chinese nationalism <label> clean <end>\n",
            "\n",
            "<sentence> chinese nationalism also often had a social component, sometimes as articulated by the political philosopher wu chuan (1746–1803) <label> clean <end>\n",
            "\n",
            "<sentence> in contrast, chinese nationalists such as jainism and buddhism have been criticized by many scholars in this and other contexts <label> clean <end>\n",
            "\n",
            "<sentence> however, it is worth noting that even within this liberal tradition, there have been numerous examples of widespread participation in political movements within the country <label> clean <end>\n",
            "\n",
            "<sentence> for example, chinese democracy is frequently said to be the \"most democratic\" of the five major forms of modern democracy <label> clean <end>\n",
            "\n",
            "<sentence> another popular statement often presented as proof of its \"democratic\" character is \"china is the most democratic country in the world\" <label> clean <end>\n",
            "\n",
            "<sentence> this statement is also presented as the conclusion that socialism means only in \"changsha\" <label> clean <end>\n",
            "\n",
            "<sentence> however, it is clear that the \"chinese democracy\" concept has been applied to several other countries, including the republic of the confederation of republics of japan, the hong kong peninsula, and the central and eastern europe <label> clean <end>\n",
            "\n",
            "<sentence> the \"chinese democratic movement\" is currently the largest political force in the country <label> clean <end>\n",
            "\n",
            "<sentence> its influence has been expressed through the movement, which has also been called \"the constitution of an era,\" \"a moral doctrine for a time,\" or \"the greatest political movement of the modern era\" <label> clean <end>\n",
            "\n",
            "<sentence> this ideology, while it has been said that \"democracy is the greatest social science since the common people are their own representatives,\" and that \"democracy provides its own moral compass,\" is also said to be the only \"socialist organization\" <label> clean <end>\n",
            "\n",
            "<sentence> while these claims may be true, it also remains important not to be misleading <label> clean <end>\n",
            "\n",
            "<sentence> while it is true that democracy is necessary in order to have a truly democratic society, many have argued that this does not mean that the system of democracy necessarily has to include a socialist system, as democracy would be much more effective for achieving social justice than a socialist one <label> clean <end>\n",
            "\n",
            "<sentence> in addition, some have argued that economic democracy and other principles such as equality of opportunity should also be present in the economic system <label> clean <end>\n",
            "\n",
            "<sentence> while this is true, it is equally true that democratic socialism should be implemented in a socialist economy <label> clean <end>\n",
            "\n",
            "<sentence> in the west, the democratic socialist movement is generally defined as \"chinese politics united in its goal of democratic socialist society and governed by one or more democratic socialist organs of governance\" <label> clean <end>\n",
            "\n",
            "<sentence> it is a political movement that promotes reform through democratic reforms, with the aim of promoting \"socialism within human society\" <label> clean <end>\n",
            "\n",
            "<sentence> this movement was primarily based on a combination of socialism (including socialism through a social federation) and democracy (including democratic socialist institutions and democratic government) <label> clean <end>\n",
            "\n",
            "<sentence> some supporters of the democratic socialist movement, particularly reformists and communists but also socialists, have argued that it has to be applied to a socialist economy <label> clean <end>\n",
            "\n",
            "<sentence> however, there also exist critics of democratic socialism, who claim that democratic socialists, like socialists, are opposed to capitalism, which they also call socialism <label> clean <end>\n",
            "\n",
            "<sentence> while many advocates of democratic socialism do indeed defend the idea of socialism, the term can be used in different ways and different contexts by different individuals <label> clean <end>\n",
            "\n",
            "<sentence> this is particularly true of the idea of socialism as a \"democratic\" economic system, since this concept can also be applied to the socialist social form of society, and socialism as\n",
            "\n",
            "[205 | 153.02] loss=2.39 avg=3.07\n",
            "[210 | 155.62] loss=3.89 avg=3.09\n",
            "[215 | 158.21] loss=2.17 avg=3.06\n",
            "[220 | 160.83] loss=4.17 avg=3.10\n",
            "[225 | 163.45] loss=2.03 avg=3.07\n",
            "[230 | 166.03] loss=1.80 avg=3.03\n",
            "[235 | 168.63] loss=4.96 avg=3.08\n",
            "[240 | 171.24] loss=4.15 avg=3.11\n",
            "[245 | 173.85] loss=4.13 avg=3.14\n",
            "[250 | 176.44] loss=4.04 avg=3.16\n",
            "[255 | 179.04] loss=3.53 avg=3.17\n",
            "[260 | 181.67] loss=3.93 avg=3.19\n",
            "[265 | 184.31] loss=2.19 avg=3.16\n",
            "[270 | 186.92] loss=4.44 avg=3.19\n",
            "[275 | 189.53] loss=3.73 avg=3.21\n",
            "[280 | 192.14] loss=2.45 avg=3.19\n",
            "[285 | 194.74] loss=3.82 avg=3.20\n",
            "[290 | 197.37] loss=4.48 avg=3.23\n",
            "[295 | 199.98] loss=2.78 avg=3.22\n",
            "[300 | 202.59] loss=1.88 avg=3.19\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-300\n",
            "[305 | 212.22] loss=2.21 avg=3.17\n",
            "[310 | 214.83] loss=4.24 avg=3.19\n",
            "[315 | 217.43] loss=2.57 avg=3.18\n",
            "[320 | 220.05] loss=2.38 avg=3.16\n",
            "[325 | 222.68] loss=4.04 avg=3.18\n",
            "[330 | 225.33] loss=2.12 avg=3.16\n",
            "[335 | 227.97] loss=3.93 avg=3.18\n",
            "[340 | 230.57] loss=4.02 avg=3.19\n",
            "[345 | 233.22] loss=3.22 avg=3.19\n",
            "[350 | 235.85] loss=3.71 avg=3.20\n",
            "[355 | 238.46] loss=3.48 avg=3.21\n",
            "[360 | 241.07] loss=4.00 avg=3.22\n",
            "[365 | 243.69] loss=4.15 avg=3.24\n",
            "[370 | 246.29] loss=3.91 avg=3.26\n",
            "[375 | 248.90] loss=3.97 avg=3.27\n",
            "[380 | 251.50] loss=4.96 avg=3.30\n",
            "[385 | 254.11] loss=2.16 avg=3.28\n",
            "[390 | 256.73] loss=3.61 avg=3.29\n",
            "[395 | 259.32] loss=2.09 avg=3.26\n",
            "[400 | 261.93] loss=2.00 avg=3.24\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8q_9FuxDg2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0DKLLTp0mp"
      },
      "source": [
        "# Generate with GPT-2\n",
        "Let's make sure we can save and load the model that we worked so hard on!\n",
        "\n",
        "[Generation information here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L407)\n",
        "\n",
        "Run this after training a model and restarting the instance. This will demonstrate that the model is saved and working."
      ]
    }
  ]
}