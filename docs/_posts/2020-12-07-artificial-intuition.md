---
layout: default
title: "Artificial Intelligence or Artificial Intuition?"
date: 2020-12-07
description: Intuition is a better description of deep neural networks
categories: [Deep-Learning, Singularity]
---

# Intuition vs Intelligence

Intuition has the following definitions:

- The power or faculty of attaining to direct knowledge or cognition without evident rational thought and inference
- Immediate apprehension or cognition
- Quick and ready insight
- The ability to understand something immediately, without the need for conscious reasoning

These definitions could be describing deep learning and machine learning in general. To date, no AI model has demonstrated anything remotely close to thought or cognition. 
This despite the fact that AI has been able to solve incredibly complex problems. 

The definitions of intelligence are:

- The ability to acquire and apply knowledge and skills, to understand
- The ability to learn or understand or to deal with new or trying situations
- The ability to apply knowledge to manipulate one's environment or to think abstractly as measured by objective criteria (such as tests)

I can imagine an argument that GPT-3 is getting closer to the ability to improvise. 
We are working towards "zero shot" task success, where we have a pretrained model that is capable of handling a variety of tasks it was not explicitly trained for. 
In the most basic terms this is called **generalization**. Does that equate to Artificial **General** Intelligence? In my opinion, no. 

If GPT-3 spits out an interesting article about life and death, you cannot ask GPT-3 why it wrote what it did. 
It has no memory of its past emissions and no concept of you as a human. It is merely an intuitive machine that has the ability to regurgitate fascinating patterns that happen to have meaning to us. 

To play devil's advocate with myself, you can show GPT-3 entirely new sets of information and problems and it seems to be able to learn rather quickly. This is not so different from humans. 
With instruction, demonstration, we can rapidly gain new abilities. But still, GPT-3 demonstrates no obvious ability to reflect on what it's learned or explain why it can do what it does.

# The need for explicability

> You do not truly understand something unless you can explain it to your grandmother

This quote is often misattributed to Einstein but it's more likely to have been said in some other permutation by Lord Nelson. The point remains; if you can't explain something,
the value of your knowledge and ability is diminished. That's not to say that intuition is worthless - on the contrary - we rely heavily on intuition all day every day.
Consider the words you're reading on this blog. Do you have to examine each letter to extract meaning and consciously assemble words and sentences and rationally parse out my intent?
Absolutely not. You can read this at lightning speed by relying on your intution. Your eyes and brain scan the page and ingest the meaning and it just sort of pops into your head.
At least, that's what reading is like for me. Only when we read very difficult or novel material do we have to fully engage our conscious thought to parse and integrate new meaning. 

> Hey GPT-3 can you explain why you said that?

We instinctively want to treat anything intelligent with agency. Asking GPT-3 why it believes what it said is anthropomorphizing a collection of weights and biases and connections. 
This implies that we expect intelligent agents to remember their own history, thus being able to account for what they said and why. Humans are fallible in this realm as well. 
We often act on instinct and it often requires deep introspection to truly understand why we do and say the things we say. I would go so far as to say that many people are 
totally unable to ascertain the truth of their beliefs due to the complex interaction of identity, emotions, and neurology. We often have different competing ideas about ourselves
and the world in which we live, and different regions of the brain are responsible for integrating all of these different parts.

GPT-3 has no such machinery or ability. No neural network has. 

Tesla autopilot merely records observations and decisions. This is, in effect, a record explaining why accidents and mishaps happen. But this data requires expert interpretation. 
The vehicle has no agency or accountability unto itself.

# The need for agency

Neither GPT-3 nor Tesla can decide of their own accord to go learn new things or test their own ability. They are idle tools that only respond to human requests. 
We can subject a deep neural network to an image or body of text and it will produce some kind of output and then it will go dormant again.

This, of course, has been the nature of machines and tools forever. From sticks and stone tools to assembly line robots, the objects and software we create has no agency, no self-direction.
The first person killed by a robot was a man named Robert Williams at a Ford plant in Michigan. Do you punish the robot? Decomission it? Reprogram it?

King Xerxes once had the ocean whipped for destroying his bridge. The Code of Hamurabi said that if a building collapses and kills people, the builder should then be put to death. 
In the first case, Xerxes may have wanted to whip the gods responsible for the wind and the waves, and settled for the next best thing. Hamurabi stayed a bit more grounded, 
asserting the engineer responsible for the deadly disaster was at fault. 

All these are fine philosophical and legal questions but I'm not as concerned about those. I'm strictly concerned about intelligence.

> Is agency necessary for intelligence?

My intuition says yes. Before I can consider a machine truly intelligent, I feel like it has to keep track of some kind of personal narrative. It has to remember what it did and said and why.
It also needs to remember all of its interactions with me. Lastly, I feel like it needs a certain degree of autonomy, the ability to self-direct and explore. 

# Isn't that just sentience? Or consciousness?

The strictest definition of consciousness is:

> The state of being awake and aware of one's surroundings.

I would argue that a Tesla satisfies this definition. But so what? A Tesla is a lot more than just a deep neural network. It is a collection of hardware and software. 
It relies on a large number of neural networks and other kinds of adaptive models. To borrow from machine learning verbiage: it is an **ensemble**. Collectively, a Tesla 
can solve simple problems with autonomy. It can negotiate roads and traffic and can navigate from one place to another. 
It also records its sensory information as well as its internal state, its evaluations and reasons for actions. For these reasons, I would suggest that a Tesla is a far more advanced
robot than GPT-3. 

As sophisticated as a self-driving Tesla is, it still only relies on the intuitions of deep neural networks. Cameras deliver images to a collection of software models that build 3D
maps using SLAM and object detection inferences to identify cars, pedestrians, and signs. These services each are nearly instant and have no agency. Not unlike your ability to read this page. 

# The line begins to blur

What sets us apart from intelligent machines? Couldn't you slap a metacognitive service into a Tesla with a voice interface and ask it why it was driving the way it did? 
This concept of explicability has been fictionalized in Westworld. Each of the Hosts has the ability to explain their behavior. To be fair, this was also explored years ago
in Star Trek TNG's episode *The Measure of a Man* where Data was subjected to philosophical scrutiny. 

But I digress. 

> My chief point here is that deep neural networks are merely intution machines. Not intelligent machines. 

